# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bYCg03V2tqZh3LU68dcwK04v69I93UhN
"""

from google.colab import drive
import pandas as pd

drive.mount("/content/drive")

df = pd.read_csv("/content/drive/My Drive/Colab Notebooks/data/dataset.csv")

df.head(5)

df.describe()

#check for missing values
df.info()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce')
df['TotalCharges'] = df['TotalCharges'].astype("float")

#Unique values in each categorical variable:
df["PaymentMethod"].nunique()
df["PaymentMethod"].unique()
df["Contract"].nunique()
df["Contract"].unique()

#Check target variable distribution
df["Churn"].value_counts()

#Create a label encoder object
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
le = LabelEncoder()
# Label Encoding will be used for columns with 2 or less unique values

le_count = 0
for col in df.columns[1:]:
    if df[col].dtype == 'object':
      le.fit(df[col])
      df[col] = le.transform(df[col])
      le_count += 1
print('{} columns were label encoded.'.format(le_count))
df.info()

corr_matrix = df.corr()
corr_matrix

#Set and compute the Correlation Matrix:
import seaborn as sns
import matplotlib.pyplot as plt

corr = df.corr()
sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot = True, annot_kws={'size':12})
heat_map=plt.gcf()
heat_map.set_size_inches(20,15)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()

def correlation(df,threshold):
  col_corr = set()
  corr_matrix = df.corr()
  for i in range(len(corr_matrix.columns)):
    for j in range(i):
      if (abs(corr_matrix.iloc[i,j])>threshold):
        colname = corr_matrix.columns[i]
        col_corr.add(colname)
  return col_corr

df.pop('TotalCharges')

df['Churn'].value_counts().plot(kind = 'bar', title = 'Bar Graph of Non-Churners vs Churners by Count (Churn is a 1)', color = 'blue', align = 'center')
plt.show()

#  how a customer pays have to do with their churn?
_, axes = plt.subplots(1, 2, sharey=True, figsize=(10, 4)) 
sns.countplot(x='PaperlessBilling', hue='Churn',
              data=df, ax=axes[0]);
sns.countplot(x='PaymentMethod', hue='Churn',
              data=df, ax=axes[1]);

#the relationship between instances of Tech Support and Churn. 

tech_support_churn = pd.crosstab(df['TechSupport'], df['Churn'])
tech_support_churn.plot(kind = 'bar', stacked = True)

# Churn rate relative to tenure.
# Stacked bar of tenure and churn.
tenure_churn = pd.crosstab(df['tenure'], df['Churn'])
tenure_churn.plot(kind = 'bar', stacked = True)
plt.ylabel('Count')
plt.xlabel('Tenure of Subscription')
plt.title('Churn Rate Relative to Tenure of Subscription (Churned is a 1)')
plt.show()
# We can clearly see the longer a customer stays as a subscriber, the less they are likely to churn!



# Plot the distribution of observations for tenure.
sns.distplot(df['tenure']);
# It shows the max tenure is 70. This must be when the data history ends. We'll account for this in our analysis.

# See if the other products they have from this company has to do with their churn.
_, axes = plt.subplots(1, 2, sharey=True, figsize=(10, 4)) 
sns.countplot(x='PhoneService', hue='Churn',
              data=df, ax=axes[0]);
sns.countplot(x='InternetService', hue='Churn',
              data=df, ax=axes[1]);

#xgboost
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
X = df[['tenure','Contract','MonthlyCharges','Partner','MultipleLines']]
y = df.Churn

#train and test split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=56)

xgb_model = XGBClassifier(max_depth=5,learning_rate=0.08,objective='binary:logistic',n_jobs=-1)
xgb_model.fit(X_train,y_train)

#predict the model

pred = xgb_model.predict(X_test)

#Accuracy

accuracy = float(np.sum(pred==y_test))/y_test.shape[0]
print("accuracy :%f "%(accuracy))

#auc

from sklearn.metrics import roc_curve,auc,roc_auc_score

xgboost_fpr,xgboost_tpr,threshold = roc_curve(y_test,pred)

auc_xgb = auc(xgboost_fpr,xgboost_tpr)

auc_xgb

import pickle

with open('xgb_model.pkl','wb') as file:
  pickle.dump(xgb_model,file)

import xgboost as xgb
booster =  xgb_model.get_booster()
booster.save_model('xgb_model.pkl')

from google.colab import files

files.download("xgb_model.pkl")